{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "consistent-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import abs_tag_lib as at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-horizontal",
   "metadata": {},
   "source": [
    "# Abstract classification via word2vec embedding\n",
    "\n",
    "In this notebook, we attempt to classify abstracts of scientific papers. To do so, we focus on two main steps,\n",
    "\n",
    "- We train a word2vec embedder using ~20,000 abstracts collected via the Nature metadata dataset, filtered by the keyword *field theory*.\n",
    "\n",
    "- Consider a restricted dataset, that we produced in a different project (see the [link]()) where the abstract have labels from 9 possible classes, and train different neural network architectures for classification.\n",
    "\n",
    "### Goal\n",
    "\n",
    "The aim of this study is to understand whether a word embedding coupled with a neural network for classification can achieve better performance than simpler embeddings like a words counter or a tf-idf vectorizer.\n",
    "\n",
    "### Methods\n",
    "\n",
    "We use a dataset that we have extracted, analysed, and cleanded in a different project (see the [link]()). The dataset is obtained via the Nature metadata API.\n",
    "\n",
    "In the first part of the project, we train the gensim word2vec model to embed words commonly appearing in scientific papers, and particularly in papers concerning *quantum field theory*.\n",
    "\n",
    "The second stage of this project consists in training different architectures of neural networks for classifying the abstracts;\n",
    "\n",
    "- We train a CNN using the procedure described in Y. Kim, \"Convolutional Neural Networks for Sentence Classification\" [arXiv:1408.5882](https://arxiv.org/pdf/1408.5882.pdf)\n",
    "\n",
    "- We train a RNN with a similar architecture as above, mainly to compare the performance of these two settings in TensorFlow.\n",
    "\n",
    "### Results\n",
    "\n",
    "For what concerns the word2vec embedding we trained, we can judge its performance by demanding similar words to a given set of words, that we consider meaningful for the task. While the model we trained is by no means general enough to be used in production, it seems to be sufficient for our pourposes. Indeed, all the word associations made by the model seem to be reasonable.\n",
    "\n",
    "For the nn...\n",
    "\n",
    "## 1. Word2vec Embedding\n",
    "\n",
    "Let's start by training the gensim word2vec model for embedding words in a real vector space. Before training the embedder, we create a vocabulary using the abstracts of Nature papers with keyword *field theory*. Our vocabulary does not solely include single words, but we additionally search for common bigrams using the gensim Phrases class. The word2vec model is trained via the continuous bag-of-words method (CBOW).\n",
    "\n",
    "### Clean the abstract and title\n",
    "\n",
    "Let's load the database of abstracts and titles we collected form the Nature metadata corpus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "artificial-nature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17835</th>\n",
       "      <td>On the connection between the LSZ and Wightman...</td>\n",
       "      <td>The LSZ asymptotic condition and the Yang-Feld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17836</th>\n",
       "      <td>The ground state of the Bose gas</td>\n",
       "      <td>The mathematical formalism describing the Bose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17837</th>\n",
       "      <td>On the vacuum state in quantum field theory. II</td>\n",
       "      <td>We want to construct, for every local irreduci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17838</th>\n",
       "      <td>A theorem concerning the positive metric</td>\n",
       "      <td>It is proved that if the n -point correlation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17839</th>\n",
       "      <td>Geometry of Electromagnetic null field</td>\n",
       "      <td>Electromagnetic tensor field can be divided in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "17835  On the connection between the LSZ and Wightman...   \n",
       "17836                   The ground state of the Bose gas   \n",
       "17837    On the vacuum state in quantum field theory. II   \n",
       "17838           A theorem concerning the positive metric   \n",
       "17839             Geometry of Electromagnetic null field   \n",
       "\n",
       "                                                abstract  \n",
       "17835  The LSZ asymptotic condition and the Yang-Feld...  \n",
       "17836  The mathematical formalism describing the Bose...  \n",
       "17837  We want to construct, for every local irreduci...  \n",
       "17838  It is proved that if the n -point correlation ...  \n",
       "17839  Electromagnetic tensor field can be divided in...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('./datasets/dataset_field_theory.pkl')\n",
    "df.drop(columns='keywords',inplace=True)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-questionnaire",
   "metadata": {},
   "source": [
    "Firstly, we check which abstracts are not in English (there are a few that are in multiple languages, generally English/Italian/Russian). We'll get rid of them.\n",
    "\n",
    "Notice that we already know that all the items have non-null abstract and title field, since we checked before storing the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "instant-italian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are, potentially, 284 foreign abstracts.\n"
     ]
    }
   ],
   "source": [
    "abstract_foreign = df['abstract'].apply(at.is_foreign).to_numpy()\n",
    "\n",
    "number_foreign = np.sum(abstract_foreign)\n",
    "print('There are, potentially, {} foreign abstracts.'.format(number_foreign))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-contributor",
   "metadata": {},
   "source": [
    "Let's drop the foreign abstracts,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fitting-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "foreign_indices = df[abstract_foreign].index\n",
    "df.drop(foreign_indices,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-membership",
   "metadata": {},
   "source": [
    "Now, let's clean the abstract and title, and map them into a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "protective-experiment",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>clean abs</th>\n",
       "      <th>clean title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17835</th>\n",
       "      <td>On the connection between the LSZ and Wightman...</td>\n",
       "      <td>The LSZ asymptotic condition and the Yang-Feld...</td>\n",
       "      <td>[lsz, asymptotic, condition, yang, feldman, eq...</td>\n",
       "      <td>[connection, lsz, wightman, quantum, field, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17836</th>\n",
       "      <td>The ground state of the Bose gas</td>\n",
       "      <td>The mathematical formalism describing the Bose...</td>\n",
       "      <td>[mathematical, formalism, describing, bose, ga...</td>\n",
       "      <td>[ground, state, bose, gas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17837</th>\n",
       "      <td>On the vacuum state in quantum field theory. II</td>\n",
       "      <td>We want to construct, for every local irreduci...</td>\n",
       "      <td>[want, construct, every, local, irreducible, q...</td>\n",
       "      <td>[vacuum, state, quantum, field, theory, ii]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17838</th>\n",
       "      <td>A theorem concerning the positive metric</td>\n",
       "      <td>It is proved that if the n -point correlation ...</td>\n",
       "      <td>[proved, n, point, correlation, function, syst...</td>\n",
       "      <td>[theorem, concerning, positive, metric]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17839</th>\n",
       "      <td>Geometry of Electromagnetic null field</td>\n",
       "      <td>Electromagnetic tensor field can be divided in...</td>\n",
       "      <td>[electromagnetic, tensor, field, divided, thre...</td>\n",
       "      <td>[geometry, electromagnetic, null, field]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "17835  On the connection between the LSZ and Wightman...   \n",
       "17836                   The ground state of the Bose gas   \n",
       "17837    On the vacuum state in quantum field theory. II   \n",
       "17838           A theorem concerning the positive metric   \n",
       "17839             Geometry of Electromagnetic null field   \n",
       "\n",
       "                                                abstract  \\\n",
       "17835  The LSZ asymptotic condition and the Yang-Feld...   \n",
       "17836  The mathematical formalism describing the Bose...   \n",
       "17837  We want to construct, for every local irreduci...   \n",
       "17838  It is proved that if the n -point correlation ...   \n",
       "17839  Electromagnetic tensor field can be divided in...   \n",
       "\n",
       "                                               clean abs  \\\n",
       "17835  [lsz, asymptotic, condition, yang, feldman, eq...   \n",
       "17836  [mathematical, formalism, describing, bose, ga...   \n",
       "17837  [want, construct, every, local, irreducible, q...   \n",
       "17838  [proved, n, point, correlation, function, syst...   \n",
       "17839  [electromagnetic, tensor, field, divided, thre...   \n",
       "\n",
       "                                             clean title  \n",
       "17835  [connection, lsz, wightman, quantum, field, th...  \n",
       "17836                         [ground, state, bose, gas]  \n",
       "17837        [vacuum, state, quantum, field, theory, ii]  \n",
       "17838            [theorem, concerning, positive, metric]  \n",
       "17839           [geometry, electromagnetic, null, field]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the stemmer and the corpus of stopwords\n",
    "wnl = WordNetLemmatizer()\n",
    "stp_en = stopwords.words('english')\n",
    "\n",
    "# clean the abstract and title\n",
    "df['clean abs'] = df['abstract'].apply(at.clean_text_lemmatize,args=(wnl,stp_en))\n",
    "df['clean title'] = df['title'].apply(at.clean_text_lemmatize,args=(wnl,stp_en))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-channel",
   "metadata": {},
   "source": [
    "### Create a vocabulary with bigrams\n",
    "\n",
    "We now use gensim object Phrases to identify the most common/meaningful bigram in the abstract/titles and create a vocabulary of 1 and 2-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "superior-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['clean abs'].to_list() + df['clean title'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-drink",
   "metadata": {},
   "source": [
    "Let's find the most common bigram in the text and add them to the vocabilary,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "single-grocery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'long', b'range'),\n",
       " (b'shed', b'light'),\n",
       " (b'schwinger', b'keldysh'),\n",
       " (b'renormalization', b'group'),\n",
       " (b'brane', b'web'),\n",
       " (b'condensed', b'matter'),\n",
       " (b'degree', b'freedom'),\n",
       " (b'boundary', b'condition'),\n",
       " (b'previous', b'work'),\n",
       " (b'spin', b'chain')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all the bigram that appear at least 20 times and have a score above 55\n",
    "phrases = Phrases(sentences, min_count=20,threshold=55)\n",
    "\n",
    "# Since we do not plan to train phrases again, we can reduce it's memory need with the following\n",
    "trained_phrases = Phraser(phrases)\n",
    "\n",
    "# Examples of bigram found\n",
    "list_bigrams = list(trained_phrases.phrasegrams.keys())\n",
    "list_bigrams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-blackberry",
   "metadata": {},
   "source": [
    "Save the trained phrases,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_phrases.save('./vocabulary/phrases')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-phase",
   "metadata": {},
   "source": [
    "### Training word2vec\n",
    "\n",
    "We now train the embedder word2vec with the title and abstracts of every paper obtianed when querying the Nature metadata database with keyword *field theory*.\n",
    "\n",
    "First we construct the sentences with common bigrams fused together,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eight-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_sentences = trained_phrases[sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-aircraft",
   "metadata": {},
   "source": [
    "Now we can train the model; we use the standard settings as we found this to work best,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sufficient-puzzle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32629752, 35945940)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cpu = multiprocessing.cpu_count()\n",
    "\n",
    "# Initialize model\n",
    "w2v_model = Word2Vec(workers=n_cpu-1)\n",
    "\n",
    "# build the vocabulary\n",
    "w2v_model.build_vocab(bi_sentences)\n",
    "\n",
    "# train the model\n",
    "w2v_model.train(bi_sentences,\n",
    "                total_examples=w2v_model.corpus_count,\n",
    "                epochs=30\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-haiti",
   "metadata": {},
   "source": [
    "We can test the model on some words that we expect be important in distinguishing between abstracts,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "affiliated-accounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test word is \"computer\"\n",
      "Similar words:\n",
      "- \"automated\" : 0.56\n",
      "- \"neuroimaging\" : 0.54\n",
      "- \"code\" : 0.53\n",
      "- \"practical\" : 0.52\n",
      "- \"deterministic\" : 0.52\n",
      "---\n",
      "Test word is \"quantum\"\n",
      "Similar words:\n",
      "- \"axiomatic\" : 0.53\n",
      "- \"quantum_mechanical\" : 0.50\n",
      "- \"classical\" : 0.49\n",
      "- \"attribute\" : 0.47\n",
      "- \"bohmian\" : 0.45\n",
      "---\n",
      "Test word is \"field\"\n",
      "Similar words:\n",
      "- \"electrodynamics\" : 0.48\n",
      "- \"maxwell\" : 0.46\n",
      "- \"perturbation\" : 0.43\n",
      "- \"gravitation\" : 0.42\n",
      "- \"interacting\" : 0.42\n",
      "---\n",
      "Test word is \"electron\"\n",
      "Similar words:\n",
      "- \"atom\" : 0.62\n",
      "- \"exciton\" : 0.55\n",
      "- \"allowance\" : 0.54\n",
      "- \"carrier\" : 0.54\n",
      "- \"phonon\" : 0.52\n",
      "---\n",
      "Test word is \"proton\"\n",
      "Similar words:\n",
      "- \"antiproton\" : 0.69\n",
      "- \"nucleon\" : 0.64\n",
      "- \"neutron\" : 0.61\n",
      "- \"nucleus\" : 0.61\n",
      "- \"280_gev\" : 0.59\n",
      "---\n",
      "Test word is \"ad_cft\"\n",
      "Similar words:\n",
      "- \"holography\" : 0.58\n",
      "- \"holographic\" : 0.54\n",
      "- \"cft\" : 0.52\n",
      "- \"agt\" : 0.52\n",
      "- \"fzz\" : 0.51\n",
      "---\n",
      "Test word is \"eth\"\n",
      "Similar words:\n",
      "- \"eigenstate\" : 0.69\n",
      "- \"thermalization\" : 0.65\n",
      "- \"monotonicity\" : 0.54\n",
      "- \"relative_entropy\" : 0.50\n",
      "- \"discreteness\" : 0.49\n",
      "---\n",
      "Test word is \"gauge\"\n",
      "Similar words:\n",
      "- \"yang_mill\" : 0.66\n",
      "- \"abelian\" : 0.57\n",
      "- \"chern_simon\" : 0.53\n",
      "- \"su\" : 0.50\n",
      "- \"lorentz\" : 0.48\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "test_words = [\"computer\",\"quantum\",\"field\",\"electron\",\"proton\",\"ad_cft\",\"eth\",\"gauge\"]\n",
    "\n",
    "for word in test_words:\n",
    "\n",
    "    print('Test word is \"{}\"'.format(word))\n",
    "    \n",
    "    print('Similar words:')\n",
    "    for similar, score in w2v_model.wv.most_similar(positive=[word],topn=5):\n",
    "        print('- \"{}\" : {:.2f}'.format(similar, score))\n",
    "        \n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-encounter",
   "metadata": {},
   "source": [
    "The above makes sense, at least is the training setting that made the most sense. We can now save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "durable-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.save('./vocabulary/FTword2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-pepper",
   "metadata": {},
   "source": [
    "## 2. CNN for abstract classification\n",
    "\n",
    "We follow the procedure described in Y. Kim, \"Convolutional Neural Networks for Sentence Classification\" [arXiv:1408.5882](https://arxiv.org/pdf/1408.5882.pdf), altough we change the architecture slightly. We go in more details about the architecture later in the notebook.\n",
    "\n",
    "### Cleaning of the data\n",
    "\n",
    "Let's first load our labelled data and clean title and abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "informal-trinity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>Some remarks about the localization of states ...</td>\n",
       "      <td>For the case of a field theory with a nuclear ...</td>\n",
       "      <td>statist physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>A proof of the crossing property for two-parti...</td>\n",
       "      <td>In the framework of the ℒ. l . Z . formalism, ...</td>\n",
       "      <td>statist physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>On the connection between the LSZ and Wightman...</td>\n",
       "      <td>The LSZ asymptotic condition and the Yang-Feld...</td>\n",
       "      <td>statist physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>On the vacuum state in quantum field theory. II</td>\n",
       "      <td>We want to construct, for every local irreduci...</td>\n",
       "      <td>statist physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4685</th>\n",
       "      <td>A theorem concerning the positive metric</td>\n",
       "      <td>It is proved that if the n -point correlation ...</td>\n",
       "      <td>statist physic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "4681  Some remarks about the localization of states ...   \n",
       "4682  A proof of the crossing property for two-parti...   \n",
       "4683  On the connection between the LSZ and Wightman...   \n",
       "4684    On the vacuum state in quantum field theory. II   \n",
       "4685           A theorem concerning the positive metric   \n",
       "\n",
       "                                               abstract        keywords  \n",
       "4681  For the case of a field theory with a nuclear ...  statist physic  \n",
       "4682  In the framework of the ℒ. l . Z . formalism, ...  statist physic  \n",
       "4683  The LSZ asymptotic condition and the Yang-Feld...  statist physic  \n",
       "4684  We want to construct, for every local irreduci...  statist physic  \n",
       "4685  It is proved that if the n -point correlation ...  statist physic  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('./datasets/dataset_ft_cleaned.pkl')\n",
    "df.drop(columns=['clean abs','clean title','number of eqs'],inplace=True)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-hunger",
   "metadata": {},
   "source": [
    "Load the previously trained Phrases object,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "funky-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_phrases = Phrases.load('./vocabulary/phrases')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-springer",
   "metadata": {},
   "source": [
    "Clean the data and convert bigrams,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "plastic-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the lemmatizer and the corpus of stopwords\n",
    "wnl = WordNetLemmatizer()\n",
    "stp_en = stopwords.words('english')\n",
    "\n",
    "# Let's clean the abstract and find bigrams using the trained Phrases object\n",
    "cleaned_abstracts = df['abstract'].apply(at.clean_text_lemmatize,args=(wnl,stp_en))\n",
    "df['clean abs'] = np.array(trained_phrases[cleaned_abstracts],dtype=object)\n",
    "\n",
    "# and we do the same for the title\n",
    "cleaned_title = df['title'].apply(at.clean_text_lemmatize,args=(wnl,stp_en))\n",
    "df['clean title'] = np.array(trained_phrases[cleaned_title],dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-perry",
   "metadata": {},
   "source": [
    "Let's see the tokenized version of title and abstracts,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "inside-shannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>clean abs</th>\n",
       "      <th>clean title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>Some remarks about the localization of states ...</td>\n",
       "      <td>For the case of a field theory with a nuclear ...</td>\n",
       "      <td>statist physic</td>\n",
       "      <td>[case, field, theory, nuclear, space, test, fu...</td>\n",
       "      <td>[remark, localization, state, quantum, field, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>A proof of the crossing property for two-parti...</td>\n",
       "      <td>In the framework of the ℒ. l . Z . formalism, ...</td>\n",
       "      <td>statist physic</td>\n",
       "      <td>[framework, l, z, formalism, crossing, propert...</td>\n",
       "      <td>[proof, crossing, property, two, particle, amp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>On the connection between the LSZ and Wightman...</td>\n",
       "      <td>The LSZ asymptotic condition and the Yang-Feld...</td>\n",
       "      <td>statist physic</td>\n",
       "      <td>[lsz, asymptotic, condition, yang, feldman, eq...</td>\n",
       "      <td>[connection, lsz, wightman, quantum, field, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>On the vacuum state in quantum field theory. II</td>\n",
       "      <td>We want to construct, for every local irreduci...</td>\n",
       "      <td>statist physic</td>\n",
       "      <td>[want, construct, every, local, irreducible, q...</td>\n",
       "      <td>[vacuum, state, quantum, field, theory, ii]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4685</th>\n",
       "      <td>A theorem concerning the positive metric</td>\n",
       "      <td>It is proved that if the n -point correlation ...</td>\n",
       "      <td>statist physic</td>\n",
       "      <td>[proved, n, point, correlation, function, syst...</td>\n",
       "      <td>[theorem, concerning, positive, metric]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "4681  Some remarks about the localization of states ...   \n",
       "4682  A proof of the crossing property for two-parti...   \n",
       "4683  On the connection between the LSZ and Wightman...   \n",
       "4684    On the vacuum state in quantum field theory. II   \n",
       "4685           A theorem concerning the positive metric   \n",
       "\n",
       "                                               abstract        keywords  \\\n",
       "4681  For the case of a field theory with a nuclear ...  statist physic   \n",
       "4682  In the framework of the ℒ. l . Z . formalism, ...  statist physic   \n",
       "4683  The LSZ asymptotic condition and the Yang-Feld...  statist physic   \n",
       "4684  We want to construct, for every local irreduci...  statist physic   \n",
       "4685  It is proved that if the n -point correlation ...  statist physic   \n",
       "\n",
       "                                              clean abs  \\\n",
       "4681  [case, field, theory, nuclear, space, test, fu...   \n",
       "4682  [framework, l, z, formalism, crossing, propert...   \n",
       "4683  [lsz, asymptotic, condition, yang, feldman, eq...   \n",
       "4684  [want, construct, every, local, irreducible, q...   \n",
       "4685  [proved, n, point, correlation, function, syst...   \n",
       "\n",
       "                                            clean title  \n",
       "4681  [remark, localization, state, quantum, field, ...  \n",
       "4682  [proof, crossing, property, two, particle, amp...  \n",
       "4683  [connection, lsz, wightman, quantum, field, th...  \n",
       "4684        [vacuum, state, quantum, field, theory, ii]  \n",
       "4685            [theorem, concerning, positive, metric]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaged-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(df['keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-soldier",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "We can now apply the word2vec embedding previously trained to our title+abstracts, to obtain the input tensor for the CNN.\n",
    "\n",
    "First, let's collect the pre-trained word2vec embedding model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mighty-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "FTwv = KeyedVectors.load('./vocabulary/FTword2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-stewart",
   "metadata": {},
   "source": [
    "Now, we map each sequence of tokens into the corresponding index in the vocabulary of the word2vec model previously trained. We will additionally pad each sequence at the end, to have inputs of same dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "european-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse title and abstract together\n",
    "full_text = df['clean title']+df['clean abs']\n",
    "\n",
    "# Hash the text and pad the resulting sequence\n",
    "df['hashed text'] = at.hashed_padded_sequences(full_text,FTwv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-scale",
   "metadata": {},
   "source": [
    "### Test and training split\n",
    "\n",
    "Let's divide the dataset into test and training sets, as we have done in the other [project](),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "naked-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just so we are sure we can reproduce the splitting\n",
    "seed = 1863\n",
    "\n",
    "# split into training and test\n",
    "df_train = df.sample(frac=0.7,random_state=seed)\n",
    "df_test = df.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-consultancy",
   "metadata": {},
   "source": [
    "The training and test sets are created, where the labels are mapped into dummy variable,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "opponent-persian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has 3270 items of length 278\n",
      "The test datatset has 1402 items\n"
     ]
    }
   ],
   "source": [
    "# The predictors for training and test are the following\n",
    "X_train = np.vstack(df_train['hashed text'].to_numpy())\n",
    "X_test =  np.vstack(df_test['hashed text'].to_numpy())\n",
    "\n",
    "# The labels are mapped into a dummy vector\n",
    "dummy_encoder = LabelBinarizer()\n",
    "\n",
    "y_train = dummy_encoder.fit_transform(df_train['keywords'].to_numpy())\n",
    "y_test = dummy_encoder.fit_transform(df_test['keywords'].to_numpy())\n",
    "\n",
    "print('The training dataset has {} items of length {}'.format(*X_train.shape))\n",
    "print('The test datatset has {} items'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-joining",
   "metadata": {},
   "source": [
    "We store the training set on disk, so that we can perform CV on a different machine,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "motivated-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training set\n",
    "np.save('./datasets/vectorized/X_train',X_train)\n",
    "np.save('./datasets/vectorized/y_train',y_train)\n",
    "\n",
    "# Save the feature names\n",
    "np.save('./datasets/vectorized/labels',labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-tuning",
   "metadata": {},
   "source": [
    "Additionally, we store the embedding weigths for the word2vec model we have trained before,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "judicial-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weigths\n",
    "vocab_size, k = FTwv.vectors.shape\n",
    "embedding_weigths = np.vstack((np.zeros(k,dtype=np.float),FTwv.vectors))\n",
    "\n",
    "np.save('./datasets/vectorized/embedding_weights',embedding_weigths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-click",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "\n",
    "We follow the CNN-singlechannel construction of the paper [arXiv:1408.5882](https://arxiv.org/pdf/1408.5882.pdf), where\n",
    "\n",
    "- The input layer consists of a matrix $n \\times k$ where $n$ is the number of words in the text (in this case the title+abstract) and $k$ is the dimension of the vector space where we perform the word2vec embedding (in our case, it is $k=100$).\n",
    "\n",
    "\n",
    "- The second layer is a convolutional layer with $m$ feature maps with window of length $h$. In the paper, multiple widowns length are used (e.g. h = 2,3,4) at the same time, but we prefer using a single window. We additionally regularize the weights with a l1 penalty, to encourage an effective variable window. The output of the feature maps is given via the ReLU function.\n",
    "\n",
    "\n",
    "- After the convolution we apply a global max-pooling layer.\n",
    "\n",
    "\n",
    "- Finally, a fully connected layer with drops-out with softmax output\n",
    "\n",
    "### Hyper-parameters selection\n",
    "\n",
    "We run K-fold cross-validation on a different machine, with the following range of parameters,\n",
    "\n",
    "- `n_filters` = $[100,200,300,400,500]$\n",
    "- `window_size` = $[2,3,4,5]$\n",
    "- `dropout_prob` = $[0.25,0.5]$\n",
    "- `l1_param` = $[10^{-2},10^{-3},10^{-4},10^{-5}]$\n",
    "\n",
    "The results are shown below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "increasing-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_df = pd.read_pickle('./cross-val/cnn/cross_val_results.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:arxiv_env]",
   "language": "python",
   "name": "conda-env-arxiv_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

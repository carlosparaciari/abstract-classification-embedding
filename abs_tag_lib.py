import re
import numpy as np
import tensorflow as tf

from langdetect import detect_langs

from scipy.stats import chi2
from prettytable import PrettyTable

# check if an item in dataframe is written in english with probability above a certain treshold
def is_foreign(item,treshold=0.7):
    
    languages = detect_langs(item)
    
    for lang in languages:
        if lang.lang == 'en' and lang.prob > treshold:
            return False
    
    return True

# clean the text and use lemmatizer
def clean_text_lemmatize(item,lemmatizer,stopwords):
    
    # remove latex equations
    item = re.sub('\$+.*?\$+','',item)
    
    # tokenize and remove punctuation
    item = re.findall('[a-zA-Z0-9]+',item)
    
    # lowecase everything
    item = [word.lower() for word in item]
    
    # remove english stopwords
    item = [word for word in item if word not in stopwords]
    
    # lemmatize the words
    item = [lemmatizer.lemmatize(word) for word in item]
    
    return item

# Maps the words into the text to the corresponding index in the word2vec model
#
# NOTE: Since we later pad with 0's, and the index 0 is associated with a particular
#       word in our w2v model, we shift the index of every word by 1.
#
def hashing_trick(text,w2v):
    return np.array([w2v.vocab[word].index+1 for word in text if word in w2v.vocab])

# Maps each sentence in dataframe into a padded sequence of integer (pad is done with 0)
def hashed_padded_sequences(df_text,w2v):
    
    hashed_text = df_text.apply(hashing_trick,args=(w2v,))
    
    max_length = hashed_text.apply(len).max()
    hashed_padded_text = hashed_text.apply(lambda arr : np.pad(arr,(0,max_length-arr.size)))
    
    return hashed_padded_text

# Creates embedding layer from w2v model
#
# NOTE: padding 0's are sent to the 0 vector
#
def get_keras_embedding(w2v,masking=False):

    # get size of the vocabulary and embedding dimension
    vocab_size, k = w2v.vectors.shape
    
    # Embedding weights, each row is the vector associated to a specific word in the model
    # NOTE: The 0-th row is mapped into the 0 vector (this is the padding)
    embedding_weigths = np.vstack((np.zeros(k,dtype=np.float),w2v.vectors))

    # Create the Keras layer (it won't be trainable)
    embedding_layer = tf.keras.layers.Embedding(vocab_size+1,
                                                k,
                                                weights=[embedding_weigths],
                                                trainable=False,
                                                mask_zero=masking
                                               )
    
    return embedding_layer

# implement the McNemar test
def mc_nemar_test(model1,model2,X_test,y_test):
    
    # instances where model 1 (model 2) is correct
    correct_m1 = np.argmax(model1.predict(X_test),axis=1) == y_test
    correct_m2 = np.argmax(model2.predict(X_test),axis=1) == y_test
    
    # Conditioning on correct instances for model 1, get correct (error) instances for model 2
    c_m1 = correct_m2[correct_m1].size

    a = np.sum(correct_m2[correct_m1])
    b = c_m1 - a

    # Conditioning on error instances for model 1, get correct (error) instances for model 2
    e_m1 = correct_m2[~correct_m1].size

    c = np.sum(correct_m2[~correct_m1])
    d = e_m1 - c
    
    # save the table with the performance comparison
    table = np.array([[a,b,a+b],[c,d,c+d],[a+c,b+d,c_m1+e_m1]])
    
    # Test statistics and p-value
    X = (b-c)**2/(b+c)
    p_value = 1 - chi2.cdf(X,df=1)
    
    return p_value,table

# Print contingency matrix generated by McNemar test
def print_table(table):

    contingency_table = PrettyTable()
    contingency_table.field_names = ['M1\M2', 'C2', 'E2', 'sum row']
    
    row_names = ['C1','E1','sum col']
    
    for name,row in zip(row_names,table):
        contingency_table.add_row(np.concatenate(([name],row)))
        
    print(contingency_table)

# Print report on statistical significance of different models accuracy (using McNemar test)    
def models_comparison(models,names,X_test,y_test,alpha=0.05):

    n_models = len(models)

    print('Null hypothesis (the two tests are equivalent) is rejected for p_val < {:.3f}'.format(alpha))

    for _ in range(n_models):
        
        model1 = models.pop()
        name1 = names.pop()
        
        for model2,name2 in zip(models,names):

            # Perform McNemar test on the two models
            p_value, contingency_table = mc_nemar_test(model1,model2,X_test,y_test)

            # print the results (p_value and contingency table)
            print('---')
            print('M1 = {} vs M2 = {}\n'.format(name1,name2))
            print_table(contingency_table)
            print('')

            if p_value < alpha:
                print('Null hypothesis rejected - models are significantly different (p_val= {:.3f})'.format(p_value))
            else:
                print('Null hypothesis cannot be rejected (p_val= {:.3f})'.format(p_value))